# SQLAlchemy-Challenge
Module 10 Assignment
Brendan Golden
This assignment required us to apply detailed SQL Alchemy concepts, queries, and manipulations while intertwining them with a SQLite database to calculate and filter data in the given tables into data frames and graphs. It also asked us to design a flask app that pulls data using the jsonify dependency with three static API  and one dynamic route using the values we calculated in the initial analysis section. I used Chat GPT for clarification on a few sections of the assignment that were not covered in class at great length, as well as collaborating with Chris Rahal and Nick Ortega from the class. 
The assignment began with importing the necessary dependencies and pytjon libraries needed to conduct the analysis and connect to the provided resources; such as automap engine, session, and engine from the SQL Alchemy library. The next part of the analytical portion of the assignment required us to use the create engine function to bind the SQLite database, view, reflect, and save pathways to the two tables within the database, and finally use the session command to interact and query the two distinct data tables. 
After completing the initial connections to the database we were tasked with designing code  to extract certain dates, and values from the measurement table to answer the analytical questions and create the necessary graphs. We began by importing the datetime and timedelta functions to establish the timeframe from which to extrapolate the values from the table. After establishing the timeframe, we used basic query functions within SQL Alchemy to find the date and precipitation values within the measurement table. After we created a data frame, explicitly naming the two columns, using panda and the precipitation data we filtered from the table. We then used the date time function within pandas, dropped all of the null values, sorted the values based on the date, and created a final data frame to plot the bar graph using basic matplotlib operations. The final portion of the precipitation analysis was basic as we used the final data frame to calculate some basic summary statistics and create a simple data frame using python list properties. 
The next part was the station analysis which was more of the same as we needed to derive some required qualitative and quantitative data from the stations table using SQL Alchemy. We were forced to implement complex query functions to deduce the top  nine most active stations, in order, and the most relevant observational data for the single most active station in the table. To conclude the analysis section we queried the data and plotted a histogram using matplotlib. 
The final part of the assignment we were tasked with combining flask and jsonify, with the values that we calculated in both the analysis sections, into a working app with all of the necessary routes. We coded in the four basic routes including the home page which listed all of the other routes, and then the precipitation, station, and temperature observation one. These three api were straightforward as it consisted mostly of copying and slightly modifying the code we had already calculated in the previous section. The final pathway required us to create a dynamic pathway, in which someone could input either one start date, or a start and end date, and receive the average, max, and min temperatures observed. 



